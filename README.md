GAUR is a tool to instrument Bison grammar. Given an input grammar file, it injects code to automatically and transparently produce a log with information characterizing parse inputs. Specifically, we plan to produce information about the impact of each parser input on the application/system it is part of. This impact information is induced by the rules used to parse the input, we associate labels to each one the the parser rules and construct an overall impact based on this information.

# Installation 
Gaur requires `gcc`, `flex` and `bison`. The production of impact information production requires [pygaur](https://github.com/gquetel/pygaur)
to be installed.

After cloning this repository GAUR can be built as follows:
```
$ git clone https://github.com/gquetel/gaur.git
$ make build
```
A gaur executable should be created within the current directory. 
``` 
$ ./gaur -h
Usage: gaur [options] -l file file 
Transform a yacc grammar into an intrumentalised one.
Options: 
-d,  --dot               Produce a dot file for the input grammar
-e,  --extract           Produce a file containing all of the grammar nonterminal
-h,  --help,             Display this help and exit
-i,  --inject=FILE       Path to the prologue code to inject
-l,  --list=FILE         Path to the nonterminals semantics list
-o,  --output=FILE       Leave output to FILE
-s,  --skeleton=FILE     Path to the skeleton file to use to produce by bison to produce the parser code

If the option -o is not used, the default output grammar filename is gaur.modified.y
```

# Running

The integration of a data collector within the parser is divided into three steps: data extractions, rule labels association (by `pygaur`), and grammar instrumentation. 

## Data extraction 

This first step aims to extract information from BISON grammars, we currently extract left-hand side nonterminal names, right-hand side terminals, and alphabetic words found in action code.  
Rules are suffixed by a number that corresponds to the number of the extracted rule in its group of rules.

## Semantic similarity computation

We use the produced CSV by the extraction step file to compute semantic flags using `pygaur`. `pygaur` uses semantic similarity measures (through NLP techniques) on data that has been extracted by GAUR to output a `json` file that specifies which labels have been associated with each rule.

## Grammar instrumentalization

The output of the Python script is made to facilitate the parsing in the next phase. This next phase takes the labels inferred (in the form of flags by `pygaur`) and includes them in the parser code. This part involves two mechanisms: 
- Hijacking Bison to use a custom [skeleton](https://www.gnu.org/software/bison/manual/html_node/Decl-Summary.html#index-_0025skeleton) that places C macros at specific positions to store information about the input while parsing, and produce a log entry once parsing is done. 
- Defining the code of these macros. This is done in the `src/inject.`*` files that are injected as a prologue in the instrumentalized grammar. Each injects files defined specific characteristics to be produced by the data collector / 

To do so we use the following command: `./gaur --list filename_flags filename_grammar` where `filename_flags` corresponds to a file generated by `pygaur`. And `filename_grammar` is the grammar to instrumentalize. Some applications require a specific parser skeleton, and the option `--skeleton filepath` allows to associate the correct one.

## Other Makefile targets

### Tree generation

GAUR also allows generating a dot file corresponding to the visual representation of the given BISON grammar using the `-d` option, which results in the creation of a `output.dot` file. `make graph` can be used to display a tree for the `parse.y` grammar. 

### Corpus generation
For the semantic inference step, GAUR creates for each grammar rule a document consisting of the name in the left-hand side, terminals in the right-hand side and the literals in the action code. For pre-processing purposes, one could aim to construct a corpus of such documents. We provide a script that creates a corpus using the grammar located in the [grammar](data/grammars/). Using the `make corpus` target will generate a text file in the output directory. 